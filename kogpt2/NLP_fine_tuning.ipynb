{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwUI3eVg70y",
        "outputId": "df0b03b8-fd49-40ba-8bb5-90c8252736c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpB5wrboDtsS",
        "outputId": "9cd5eceb-251c-4cab-b036-fd7fc545c306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Github pat: ··········\n",
            "Cloning into 'nlp_fine_tuning'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 77 (delta 44), reused 56 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (77/77), 5.03 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (44/44), done.\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "token = getpass(\"Github pat: \")\n",
        "\n",
        "!git clone https://{token}@github.com/hippo7426/nlp_fine_tuning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl83ULxSeEha",
        "outputId": "dbc72239-b413-4a11-a999-fa070114b224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate, bert-score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert-score-0.3.13 evaluate-0.4.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets tokenizers accelerate evaluate bert-score pandas numpy matplotlib seaborn tqdm scikit-learn wandb tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZM3wUhUD11V",
        "outputId": "be71c6bf-deaf-4fdf-994b-ef69663fa667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nlp_fine_tuning\n"
          ]
        }
      ],
      "source": [
        "%cd nlp_fine_tuning/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJahHBnKAd7V"
      },
      "outputs": [],
      "source": [
        "# !git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFXse8ZSD4ep"
      },
      "outputs": [],
      "source": [
        "# !python main.py --head-only --trainable-layers 4 --epochs 5 --gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6mfw7ck3tpO"
      },
      "outputs": [],
      "source": [
        "# now = datetime.now()\n",
        "# date_time = now.strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "\n",
        "# base_dir = \"/content/drive/MyDrive/2025-NLP/best_models/\"\n",
        "# dst_dir = os.path.join(base_dir, f\"{date_time}\")\n",
        "# os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# shutil.copytree(\"/content/nlp_fine_tuning/saved_models/best_model\", dst_dir, dirs_exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtTNWjuNT2Y5"
      },
      "outputs": [],
      "source": [
        "# !python3 main.py --mode evaluate --model-path /content/drive/MyDrive/2025-NLP/best_models/\n",
        "# eval_dir = os.path.join(base_dir, f\"{}_eval\")\n",
        "# shutil.copytree(\"/content/nlp_fine_tuning/outputs\", eval_dir, dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdUxBdx7cC3r",
        "outputId": "47e8a47e-e395-4aa2-fc1f-a6800431e714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-09 21:30:48.651884: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-09 21:30:48.670015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749504648.692464   14878 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749504648.699214   14878 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-09 21:30:48.721331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "=== KoGPT2 Korean Poetry Fine-tuning ===\n",
            "\n",
            "Configuration:\n",
            "- Model: skt/kogpt2-base-v2\n",
            "- Device: cuda\n",
            "- Full fine-tuning: True\n",
            "- Use LoRA: False\n",
            "- Use Prefix-tuning: False\n",
            "- Use Prompt-tuning: False\n",
            "- Epochs: 3\n",
            "- Learning rate: 5e-05\n",
            "- Batch size: 4\n",
            "- Max length: 512\n",
            "\n",
            "Loading and preprocessing data...\n",
            "Loading data from data/prompt_dataset.json...\n",
            "Loaded 31024 samples\n",
            "Data split: Train=24819, Val=3102, Test=3103\n",
            "\n",
            "=== Starting Evaluation ===\n",
            "Loading model from /content/drive/MyDrive/2025-NLP/best_models/Prompt-tuning/\n",
            "Setting up tokenizer with special tokens...\n",
            "Tokenizer vocab size: 51203\n",
            "Loading PROMPT_TUNING model...\n",
            "Resizing base model embeddings from 51200 to 51203\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "✅ Model and tokenizer loaded successfully\n",
            "   Tokenizer vocab size: 51203\n",
            "   Model embedding size: 51203\n",
            "   Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
            "   Added tokens: ['<s>', '</s>', '<usr>', '<pad>', '<sys>', '<unk>', '<mask>', '<d>', '</d>', '<unused0>', '<unused1>', '<unused2>', '<unused3>', '<unused4>', '<unused5>', '<unused6>', '<unused7>', '<unused8>', '<unused9>', '<unused10>', '<unused11>', '<unused12>', '<unused13>', '<unused14>', '<unused15>', '<unused16>', '<unused17>', '<unused18>', '<unused19>', '<unused20>', '<unused21>', '<unused22>', '<unused23>', '<unused24>', '<unused25>', '<unused26>', '<unused27>', '<unused28>', '<unused29>', '<unused30>', '<unused31>', '<unused32>', '<unused33>', '<unused34>', '<unused35>', '<unused36>', '<unused37>', '<unused38>', '<unused39>', '<unused40>', '<unused41>', '<unused42>', '<unused43>', '<unused44>', '<unused45>', '<unused46>', '<unused47>', '<unused48>', '<unused49>', '<unused50>', '<unused51>', '<unused52>', '<unused53>', '<unused54>', '<unused55>', '<unused56>', '<unused57>', '<unused58>', '<unused59>', '<unused60>', '<unused61>', '<unused62>', '<unused63>', '<unused64>', '<unused65>', '<unused66>', '<unused67>', '<unused68>', '<unused69>', '<unused70>', '<unused71>', '<unused72>', '<unused73>', '<unused74>', '<unused75>', '<unused76>', '<unused77>', '<unused78>', '<unused79>', '<unused80>', '<unused81>', '<unused82>', '<unused83>', '<unused84>', '<unused85>', '<unused86>', '<unused87>', '<unused88>', '<unused89>', '<unused90>', '<unused91>', '<unused92>', '<unused93>', '<unused94>', '<unused95>', '<unused96>', '<unused97>', '<unused98>', '<unused99>', ':-)', ':)', '-)', '(-:', '(:-)', '(:-(', '-}', '8-O', \"'-)\", ':-#', ':-*', ':-/', ':->', ':-@', ':-d', ':-V', ':-X', ':-\\\\', ':-]', ';-(', '>;->', ';^)', '%-)', '):-(', '3:]', ':-&', '8:-)', ':-)8<', ':-O', ':-6', '+:-)', 'O:-)', ':-<', ':-?', ':-E', ':-Q', ':-}X', ':-[', ':-a', ':-{', ':-{}', ':^)', '<:-l', ':=)', '>:->', '>:-l', '@:-)', '@:-}', 'C=:-)', 'X:-)', '[:-)', '[:]', '{:-)', 'l^o', '}:^#)', ':-(=)', 'O-)', ':-3', ':=', ':-\"', 'P-(', '?-(', 'd:-)', ':8)', ':-7', '):-)', ':/\\\\)', '8(:-)', '([(', ':-(*)', '&-l', ':-e', ':(', ':,(', ':-(', ':-P', ':-S', ':-C', ':-r', ':-t', ':-W', 'X-(', 'l-O', 'l:-O', '$-)', ':-!', ':----}', '=:-)', '=:-(', '3:[', '8<:-)', ':#)', '8-#', 'B-)', '8-)', '|-(', 'H-)', ']-I', 'V^J', '+-(', '~:-P', \"`'\", 'L-P', 'BI', 'O|', '^^', 'ㅜㅜ', 'ㅠㅠ', 'ㅡㅡ', '😠', '👿', '😧', '😰', '😲', '😁', '🐻', '🐱', '😹', '😼', '🤡', '🥶', '😖', '😕', '🐮', '🤠', '😿', '😢', '😞', '😵', '🐶', '😓', '🐲', '🤤', '😑', '😘', '😋', '😱', '🤮', '🤭', '🤕', '😷', '🧐', '😮', '🤨', '🙄', '😤', '🤬', '😂', '🤒', '😛', '😶', '😨', '🌛', '😳', '🦊', '🐸', '☹', '☹️', '😦', '🌝', '😬', '😺', '😸', '😀', '😃', '😄', '😅', '😆', '🐹', '🐴', '🥵', '🤗', '😯', '😽', '😗', '😚', '😙', '🌜', '🦁', '😭', '🤥', '🤦🏿\\u200d♂', '🤦🏻\\u200d♂', '🤦🏾\\u200d♂', '🤦🏼\\u200d♂', '🤦🏽\\u200d♂', '🤦\\u200d♂', '🤦🏿\\u200d♂️', '🤦🏻\\u200d♂️', '🤦🏾\\u200d♂️', '🤦🏼\\u200d♂️', '🤦🏽\\u200d♂️', '🤦\\u200d♂️', '🤑', '🐵', '🐭', '🤢', '🤓', '😐', '🌚', '🐼', '🥳', '😔', '😣', '🤦', '🤦🏿', '🤦🏻', '🤦🏾', '🤦🏼', '🤦🏽', '🐷', '🥺', '😾', '😡', '🐰', '😌', '🤖', '😥', '🤫', '😴', '😪', '🙁', '🙂', '😻', '☺', '☺️', '🥰', '😇', '😍', '😈', '😊', '😎', '😏', '🤧', '😝', '🌞', '🤔', '🐯', '😫', '😒', '🦄', '🙃', '🙀', '😩', '🌬', '🌬️', '😉', '😜', '🐺', '🤦🏿\\u200d♀', '🤦🏻\\u200d♀', '🤦🏾\\u200d♀', '🤦🏼\\u200d♀', '🤦🏽\\u200d♀', '🤦\\u200d♀', '🤦🏿\\u200d♀️', '🤦🏻\\u200d♀️', '🤦🏾\\u200d♀️', '🤦🏼\\u200d♀️', '🤦🏽\\u200d♀️', '🤦\\u200d♀️', '🥴', '😟', '🥱', '🤪', '🤐', '<|endoftext|>', '<|topic:', '|>']\n",
            "Starting comprehensive evaluation...\n",
            "Calculating perplexity...\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "Perplexity: 341.64\n",
            "Generating 50 poems for evaluation...\n",
            "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1926: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
            "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
            "Calculating BERTScore...\n",
            "BERTScore - Precision: 0.566, Recall: 0.582, F1: 0.573\n",
            "\n",
            "=== Example Generations ===\n",
            "\n",
            "Example 1:\n",
            "Generated: <|topic: 사랑|> \n",
            "상기린수에서 그간 동독에서 장마르크를 기렸습니다.\n",
            "공차공차역전기와 열세하고 있다.\n",
            "수량전차역병기를 두고 있는 것은 이통역전기로 사용했으며 철도사업부와 협...\n",
            "Reference: 가만히 손을 들여다보고 있으면\n",
            "우리의 손이 언제나 욕망을 쥐는 데만\n",
            "사용되고 있다는 말도 거짓임을 압니다\n",
            "솨아솨아 작은 오솔길을 따라가 보면\n",
            "무엇을 쥐었을 때보다\n",
            "그저 흘려보낸 것...\n",
            "\n",
            "Example 2:\n",
            "Generated: <|topic: 세한도|> \n",
            "이들은 지난 7일 이고리 |>>1천년 전>1억불<03년도]>203:09년도] : 06년도=9월19098년도>10월15일 :9월15일 :8월7일 :8월3...\n",
            "Reference: 윤곽만 남은 세월 찬바람 속에\n",
            "침묵의 뼈대를 그려 넣는다\n",
            "배경을 잃은 노송이 \n",
            "굽은 팔로 거친 솔잎 한 줌을 받치고\n",
            "목숨의 여백에 뿌리를 드러내었다\n",
            "마른 붓질로 대지를 쓸고 가는 ...\n",
            "\n",
            "Example 3:\n",
            "Generated: <|topic: 소통|> \n",
            "한 ᅵ일종의 역행태 및 관계라고 한다.\n",
            "그러나 ᅵᅵ ᅵ ᅵᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ ᅵ...\n",
            "Reference: 주위가 더 어두워지면\n",
            "혼잣말하는 시간이 늘어난다.\n",
            "물상이 하나씩 보이지 않을수록\n",
            "말을 걸 대상은 풍성하게 늘어나고\n",
            "세상은 돌아서서 은밀하고 다정해진다.\n",
            "​\n",
            "내 말은 입을 떠나자 얼...\n",
            "Evaluation results saved to outputs/evaluation_results.txt\n",
            "Evaluation completed!\n",
            "\n",
            "All tasks completed!\n"
          ]
        }
      ],
      "source": [
        "!python3 main.py --mode evaluate --model-path /content/drive/MyDrive/2025-NLP/best_models/freeze-epoch5/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXDmG6djVPUY",
        "outputId": "3c15efb1-09b0-4ac0-c7cc-e239f1db5745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-09 21:48:07.599508: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-09 21:48:07.617906: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749505687.640145   19411 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749505687.646822   19411 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-09 21:48:07.668815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "=== KoGPT2 Korean Poetry Fine-tuning ===\n",
            "\n",
            "Configuration:\n",
            "- Model: skt/kogpt2-base-v2\n",
            "- Device: cuda\n",
            "- Full fine-tuning: True\n",
            "- Use LoRA: False\n",
            "- Use Prefix-tuning: False\n",
            "- Use Prompt-tuning: False\n",
            "- Epochs: 3\n",
            "- Learning rate: 5e-05\n",
            "- Batch size: 4\n",
            "- Max length: 512\n",
            "\n",
            "Loading and preprocessing data...\n",
            "Loading data from data/prompt_dataset.json...\n",
            "Loaded 31024 samples\n",
            "Data split: Train=24819, Val=3102, Test=3103\n",
            "\n",
            "=== Testing Poetry Generation ===\n",
            "Loading model from /content/drive/MyDrive/2025-NLP/best_models/freeze-epoch5/\n",
            "Setting up tokenizer with special tokens...\n",
            "Tokenizer vocab size: 51203\n",
            "Loading full model...\n",
            "✅ Model and tokenizer loaded successfully\n",
            "   Tokenizer vocab size: 51203\n",
            "   Model embedding size: 51203\n",
            "   Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
            "   Added tokens: ['<s>', '</s>', '<usr>', '<pad>', '<sys>', '<unk>', '<mask>', '<d>', '</d>', '<unused0>', '<unused1>', '<unused2>', '<unused3>', '<unused4>', '<unused5>', '<unused6>', '<unused7>', '<unused8>', '<unused9>', '<unused10>', '<unused11>', '<unused12>', '<unused13>', '<unused14>', '<unused15>', '<unused16>', '<unused17>', '<unused18>', '<unused19>', '<unused20>', '<unused21>', '<unused22>', '<unused23>', '<unused24>', '<unused25>', '<unused26>', '<unused27>', '<unused28>', '<unused29>', '<unused30>', '<unused31>', '<unused32>', '<unused33>', '<unused34>', '<unused35>', '<unused36>', '<unused37>', '<unused38>', '<unused39>', '<unused40>', '<unused41>', '<unused42>', '<unused43>', '<unused44>', '<unused45>', '<unused46>', '<unused47>', '<unused48>', '<unused49>', '<unused50>', '<unused51>', '<unused52>', '<unused53>', '<unused54>', '<unused55>', '<unused56>', '<unused57>', '<unused58>', '<unused59>', '<unused60>', '<unused61>', '<unused62>', '<unused63>', '<unused64>', '<unused65>', '<unused66>', '<unused67>', '<unused68>', '<unused69>', '<unused70>', '<unused71>', '<unused72>', '<unused73>', '<unused74>', '<unused75>', '<unused76>', '<unused77>', '<unused78>', '<unused79>', '<unused80>', '<unused81>', '<unused82>', '<unused83>', '<unused84>', '<unused85>', '<unused86>', '<unused87>', '<unused88>', '<unused89>', '<unused90>', '<unused91>', '<unused92>', '<unused93>', '<unused94>', '<unused95>', '<unused96>', '<unused97>', '<unused98>', '<unused99>', ':-)', ':)', '-)', '(-:', '(:-)', '(:-(', '-}', '8-O', \"'-)\", ':-#', ':-*', ':-/', ':->', ':-@', ':-d', ':-V', ':-X', ':-\\\\', ':-]', ';-(', '>;->', ';^)', '%-)', '):-(', '3:]', ':-&', '8:-)', ':-)8<', ':-O', ':-6', '+:-)', 'O:-)', ':-<', ':-?', ':-E', ':-Q', ':-}X', ':-[', ':-a', ':-{', ':-{}', ':^)', '<:-l', ':=)', '>:->', '>:-l', '@:-)', '@:-}', 'C=:-)', 'X:-)', '[:-)', '[:]', '{:-)', 'l^o', '}:^#)', ':-(=)', 'O-)', ':-3', ':=', ':-\"', 'P-(', '?-(', 'd:-)', ':8)', ':-7', '):-)', ':/\\\\)', '8(:-)', '([(', ':-(*)', '&-l', ':-e', ':(', ':,(', ':-(', ':-P', ':-S', ':-C', ':-r', ':-t', ':-W', 'X-(', 'l-O', 'l:-O', '$-)', ':-!', ':----}', '=:-)', '=:-(', '3:[', '8<:-)', ':#)', '8-#', 'B-)', '8-)', '|-(', 'H-)', ']-I', 'V^J', '+-(', '~:-P', \"`'\", 'L-P', 'BI', 'O|', '^^', 'ㅜㅜ', 'ㅠㅠ', 'ㅡㅡ', '😠', '👿', '😧', '😰', '😲', '😁', '🐻', '🐱', '😹', '😼', '🤡', '🥶', '😖', '😕', '🐮', '🤠', '😿', '😢', '😞', '😵', '🐶', '😓', '🐲', '🤤', '😑', '😘', '😋', '😱', '🤮', '🤭', '🤕', '😷', '🧐', '😮', '🤨', '🙄', '😤', '🤬', '😂', '🤒', '😛', '😶', '😨', '🌛', '😳', '🦊', '🐸', '☹', '☹️', '😦', '🌝', '😬', '😺', '😸', '😀', '😃', '😄', '😅', '😆', '🐹', '🐴', '🥵', '🤗', '😯', '😽', '😗', '😚', '😙', '🌜', '🦁', '😭', '🤥', '🤦🏿\\u200d♂', '🤦🏻\\u200d♂', '🤦🏾\\u200d♂', '🤦🏼\\u200d♂', '🤦🏽\\u200d♂', '🤦\\u200d♂', '🤦🏿\\u200d♂️', '🤦🏻\\u200d♂️', '🤦🏾\\u200d♂️', '🤦🏼\\u200d♂️', '🤦🏽\\u200d♂️', '🤦\\u200d♂️', '🤑', '🐵', '🐭', '🤢', '🤓', '😐', '🌚', '🐼', '🥳', '😔', '😣', '🤦', '🤦🏿', '🤦🏻', '🤦🏾', '🤦🏼', '🤦🏽', '🐷', '🥺', '😾', '😡', '🐰', '😌', '🤖', '😥', '🤫', '😴', '😪', '🙁', '🙂', '😻', '☺', '☺️', '🥰', '😇', '😍', '😈', '😊', '😎', '😏', '🤧', '😝', '🌞', '🤔', '🐯', '😫', '😒', '🦄', '🙃', '🙀', '😩', '🌬', '🌬️', '😉', '😜', '🐺', '🤦🏿\\u200d♀', '🤦🏻\\u200d♀', '🤦🏾\\u200d♀', '🤦🏼\\u200d♀', '🤦🏽\\u200d♀', '🤦\\u200d♀', '🤦🏿\\u200d♀️', '🤦🏻\\u200d♀️', '🤦🏾\\u200d♀️', '🤦🏼\\u200d♀️', '🤦🏽\\u200d♀️', '🤦\\u200d♀️', '🥴', '😟', '🥱', '🤪', '🤐', '<|endoftext|>', '<|topic:', '|>']\n",
            "Generated poems:\n",
            "==================================================\n",
            "🤖 Model info:\n",
            "   Type: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
            "   Device: cuda:0\n",
            "   Embedding size: 51203\n",
            "🔤 Tokenizer info:\n",
            "   Vocab size: 51203\n",
            "   Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}\n",
            "==================================================\n",
            "\n",
            "Topic: 자연\n",
            "------------------------------\n",
            "🎯 Generating poem for topic: '자연'\n",
            "📝 Prompt: '<|topic:자연|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 204])\n",
            "📖 Generated text length: 408 chars\n",
            "🎭 Extracted poem length: 408 chars\n",
            "<|topic: 자연|> \n",
            "- 연화리 시편 3\n",
            "​\n",
            "내 눈 속을 파고드는\n",
            "물결처럼\n",
            "그대 속에 들어와\n",
            "물결을 일으키며\n",
            "눈 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에 들어와\n",
            "그대 속에\n",
            "------------------------------\n",
            "\n",
            "Topic: 사랑\n",
            "------------------------------\n",
            "🎯 Generating poem for topic: '사랑'\n",
            "📝 Prompt: '<|topic:사랑|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 62])\n",
            "📖 Generated text length: 95 chars\n",
            "🎭 Extracted poem length: 95 chars\n",
            "<|topic: 사랑|> \n",
            "꽃잎 하나\n",
            "​\n",
            "저물 무렵\n",
            "​\n",
            "물결에 \n",
            "그때,\n",
            "​\n",
            "​\n",
            "내 마음이\n",
            "​\n",
            "어느새\n",
            "​\n",
            "​\n",
            "아아,\n",
            "​\n",
            "​\n",
            "그것이\n",
            "​\n",
            "​\n",
            "​\n",
            "너를\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Topic: 그리움\n",
            "------------------------------\n",
            "🎯 Generating poem for topic: '그리움'\n",
            "📝 Prompt: '<|topic:그리움|>'\n",
            "🔧 Input tokens: torch.Size([1, 5])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 97])\n",
            "📖 Generated text length: 200 chars\n",
            "🎭 Extracted poem length: 200 chars\n",
            "<|topic: 그리움|> \n",
            "누군가 말했다\n",
            "​\n",
            "당신이 세상을 떠났다\n",
            "​\n",
            "그대가 당신을 떠날 때\n",
            "나는 당신을 떠나보낸 것이었다\n",
            "​\n",
            "그대는 내게 눈물로\n",
            "서랍을 적셔주었다\n",
            "​\n",
            "당신이 나를 떠난 후,\n",
            "나는 당신에게 그대를 용서해달라고 부탁했다\n",
            "​\n",
            "나는\n",
            "당신의 가슴에 손을 대보았지만\n",
            "그대가 오지 않은 것은\n",
            "​\n",
            "당신이 나를 떠나보낸 것이었다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Topic: 가을\n",
            "------------------------------\n",
            "🎯 Generating poem for topic: '가을'\n",
            "📝 Prompt: '<|topic:가을|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 31])\n",
            "📖 Generated text length: 67 chars\n",
            "🎭 Extracted poem length: 67 chars\n",
            "<|topic: 가을|> \n",
            "늦가을 저녁 무렵\n",
            "어디선가 물소리\n",
            "물소리\n",
            "​\n",
            "그냥\n",
            "가을이 오는 소리\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Topic: 달빛\n",
            "------------------------------\n",
            "🎯 Generating poem for topic: '달빛'\n",
            "📝 Prompt: '<|topic:달빛|>'\n",
            "🔧 Input tokens: torch.Size([1, 5])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 205])\n",
            "📖 Generated text length: 412 chars\n",
            "🎭 Extracted poem length: 412 chars\n",
            "<|topic: 달빛|> \n",
            "달이 뜨면 달이 뜨지 않아도\n",
            "달이 뜨면\n",
            "달이 뜨면\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "달이 뜨지 않아도\n",
            "------------------------------\n",
            "\n",
            "=== Interactive Generation ===\n",
            "Enter topics to generate poems (type 'quit' to exit):\n",
            "\n",
            "Enter topic: 사랑\n",
            "\n",
            "Generating poem for '사랑'...\n",
            "🎯 Generating poem for topic: '사랑'\n",
            "📝 Prompt: '<|topic:사랑|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 94])\n",
            "📖 Generated text length: 195 chars\n",
            "🎭 Extracted poem length: 195 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑|> \n",
            "아무도 없는 빈집에\n",
            "내 몸만 혼자 남아\n",
            "아직도 나를 사랑하는지\n",
            "​\n",
            "나는 늘 그 방을 지키고 싶어\n",
            "이른 봄 아침부터\n",
            "내 몸만을 사랑하지 않는다\n",
            "​\n",
            "아무도 없는 빈집은\n",
            "내가 사랑하는 이여\n",
            "빈집의 안방을 지키며\n",
            "​\n",
            "그렇지만 나는\n",
            "내가 사랑하고 있는 이여\n",
            "​\n",
            "나를 사랑하지 않는다\n",
            "사랑하지 않는 내 몸만\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 사랑\n",
            "\n",
            "Generating poem for '사랑'...\n",
            "🎯 Generating poem for topic: '사랑'\n",
            "📝 Prompt: '<|topic:사랑|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 72])\n",
            "📖 Generated text length: 154 chars\n",
            "🎭 Extracted poem length: 154 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑|> \n",
            "내 안에 갇힌 것이\n",
            "내 안에 갇힌 것이다\n",
            "​\n",
            "내 안에 갇힌 것이\n",
            "내 안에 갇힌 것이다\n",
            "​\n",
            "내가 갇힌 것은\n",
            "내가 나를\n",
            "갇은 것이다\n",
            "​\n",
            "내 안에 갇힌 것은\n",
            "내가 나를\n",
            "갇은 것이다\n",
            "​\n",
            "내가 나를\n",
            "갇은 것은\n",
            "내가 나를\n",
            "갇힌 것이다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 사랑\n",
            "\n",
            "Generating poem for '사랑'...\n",
            "🎯 Generating poem for topic: '사랑'\n",
            "📝 Prompt: '<|topic:사랑|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 204])\n",
            "📖 Generated text length: 366 chars\n",
            "🎭 Extracted poem length: 366 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑|> \n",
            "사랑은\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "어찌하여\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은 것을 꿈꾸었기에\n",
            "얼마나 많은\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 사랑\n",
            "\n",
            "Generating poem for '사랑'...\n",
            "🎯 Generating poem for topic: '사랑'\n",
            "📝 Prompt: '<|topic:사랑|>'\n",
            "🔧 Input tokens: torch.Size([1, 4])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 95])\n",
            "📖 Generated text length: 195 chars\n",
            "🎭 Extracted poem length: 195 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑|> \n",
            "당신은 저 여자를 사랑한 적이 있나요?\n",
            "아니면 당신이 당신을 사랑했던 적이 있나요?\n",
            "​\n",
            "나는 당신을 사랑했으므로\n",
            "당신은 저 여자를 사랑했네\n",
            "​\n",
            "그때는 당신 때문에 당신을 사랑하는 법을 배웠지요\n",
            "​\n",
            "사랑한다는 것은\n",
            "당신이 내게 두 번\n",
            "당신의 두 번째를\n",
            "​\n",
            "그렇게 사랑했네\n",
            "​\n",
            "당신이 내게 두 번째를\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 사랑하는 가족\n",
            "\n",
            "Generating poem for '사랑하는 가족'...\n",
            "🎯 Generating poem for topic: '사랑하는 가족'\n",
            "📝 Prompt: '<|topic:사랑하는 가족|>'\n",
            "🔧 Input tokens: torch.Size([1, 5])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 63])\n",
            "📖 Generated text length: 141 chars\n",
            "🎭 Extracted poem length: 141 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑하는 가족|> \n",
            "그날, 그날의 첫사랑은\n",
            "아무도 모르게 사라진다\n",
            "​\n",
            "아무도 모르게 사라진다\n",
            "​\n",
            "어느 날 그날의 첫사랑은\n",
            "자기도 모르게 사라진다\n",
            "​\n",
            "마음에 습기가 많아진 나무는\n",
            "아무도 모르게\n",
            "​\n",
            "그렇게 사라진다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 밥을 먹는 가족\n",
            "\n",
            "Generating poem for '밥을 먹는 가족'...\n",
            "🎯 Generating poem for topic: '밥을 먹는 가족'\n",
            "📝 Prompt: '<|topic:밥을 먹는 가족|>'\n",
            "🔧 Input tokens: torch.Size([1, 6])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 44])\n",
            "📖 Generated text length: 100 chars\n",
            "🎭 Extracted poem length: 100 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 밥을 먹는 가족|> \n",
            "사랑이 식어가는 봄밤\n",
            "밥 먹으러 나가보면\n",
            "아직 오지 않은 아이들이\n",
            "아직 돌아오지 않아\n",
            "식어가는 봄밤이 오길 기다린다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 가족들과 밥\n",
            "\n",
            "Generating poem for '가족들과 밥'...\n",
            "🎯 Generating poem for topic: '가족들과 밥'\n",
            "📝 Prompt: '<|topic:가족들과 밥|>'\n",
            "🔧 Input tokens: torch.Size([1, 6])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 206])\n",
            "📖 Generated text length: 376 chars\n",
            "🎭 Extracted poem length: 375 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 가족들과 밥|> \n",
            "당신이 오시면\n",
            "당신이 오시는 걸 보실까요\n",
            "당신이 오시는 걸 보실까요\n",
            "당신이 오시는 걸 보실까요\n",
            "그런데 그건 당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "당신이 오시기 전에\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다 위로 떠오르는 태양\n",
            "\n",
            "Generating poem for '바다 위로 떠오르는 태양'...\n",
            "🎯 Generating poem for topic: '바다 위로 떠오르는 태양'\n",
            "📝 Prompt: '<|topic:바다 위로 떠오르는 태양|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 66])\n",
            "📖 Generated text length: 131 chars\n",
            "🎭 Extracted poem length: 131 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다 위로 떠오르는 태양|> \n",
            "그때나 지금이나\n",
            "어느 한순간도\n",
            "한낮에도\n",
            "바다를 향해 솟아오르는 태양의\n",
            "한 자락 흔들리는\n",
            "​\n",
            "내 한몸이\n",
            "이제는\n",
            "​\n",
            "나도 한몸이 되어\n",
            "바다를 향해 솟아오를 수 있기를\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 자전거를 타는 고양이\n",
            "\n",
            "Generating poem for '자전거를 타는 고양이'...\n",
            "🎯 Generating poem for topic: '자전거를 타는 고양이'\n",
            "📝 Prompt: '<|topic:자전거를 타는 고양이|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 103])\n",
            "📖 Generated text length: 234 chars\n",
            "🎭 Extracted poem length: 234 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 자전거를 타는 고양이|> \n",
            "내가 사는 아파트에는 늘 자전거가 있는 법이다\n",
            "​\n",
            "자전거와 함께 산다는 것은\n",
            "한때는 아무것도 아니었다\n",
            "​\n",
            "나는 늘\n",
            "자전거를 타고 다니지만\n",
            "내가 사는 아파트는\n",
            "아직\n",
            "그 어떤 것도 아닌\n",
            "나의 집으로 향하는\n",
            "​\n",
            "이런 나의 집,\n",
            "​\n",
            "그러나 나의\n",
            "자전거는\n",
            "나를 조금씩 작아지고\n",
            "자전거를 탈 줄 아는 것이다\n",
            "​\n",
            "나는 지금 나의 집으로\n",
            "자전거를 타고 다니고 있다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다 위로 지는 태양\n",
            "\n",
            "Generating poem for '바다 위로 지는 태양'...\n",
            "🎯 Generating poem for topic: '바다 위로 지는 태양'\n",
            "📝 Prompt: '<|topic:바다 위로 지는 태양|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 60])\n",
            "📖 Generated text length: 128 chars\n",
            "🎭 Extracted poem length: 128 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다 위로 지는 태양|> \n",
            "마른 배 한 척이\n",
            "저물녘까지 따라왔다\n",
            "​\n",
            "바다에 닿을 때까지\n",
            "저무는 일이 없었다\n",
            "​\n",
            "저무는 일은 없었다\n",
            "​\n",
            "저무는 일이 없었다\n",
            "​\n",
            "저무는 일 없는 날들이 많았다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다 위에 뜬 달\n",
            "\n",
            "Generating poem for '바다 위에 뜬 달'...\n",
            "🎯 Generating poem for topic: '바다 위에 뜬 달'\n",
            "📝 Prompt: '<|topic:바다 위에 뜬 달|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 64])\n",
            "📖 Generated text length: 128 chars\n",
            "🎭 Extracted poem length: 128 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다 위에 뜬 달|> \n",
            "바다의 물결에 떠내려가는 달이\n",
            "수평선 너머로\n",
            "떨어져 내리는 달\n",
            "​\n",
            "저렇게 멀리 떠내려갈 수 있을까\n",
            "​\n",
            "달빛이 밀려와\n",
            "한없이 깊고\n",
            "​\n",
            "저렇게 멀리 떠내려갈 수 있을까\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 사랑하는 고양이\n",
            "\n",
            "Generating poem for '사랑하는 고양이'...\n",
            "🎯 Generating poem for topic: '사랑하는 고양이'\n",
            "📝 Prompt: '<|topic:사랑하는 고양이|>'\n",
            "🔧 Input tokens: torch.Size([1, 5])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 60])\n",
            "📖 Generated text length: 134 chars\n",
            "🎭 Extracted poem length: 134 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑하는 고양이|> \n",
            "눈 내리는 날\n",
            "마음이 먼저 젖는다\n",
            "눈 내리는 날은\n",
            "그대에게\n",
            "그대에게\n",
            "내 마음의 창문을 열어줘요\n",
            "눈 내리는 날은\n",
            "그대에게\n",
            "나의 창문을 열어줘요\n",
            "마음 안에\n",
            "내 마음의 창을 열어줘요\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 사랑하는 동물\n",
            "\n",
            "Generating poem for '사랑하는 동물'...\n",
            "🎯 Generating poem for topic: '사랑하는 동물'\n",
            "📝 Prompt: '<|topic:사랑하는 동물|>'\n",
            "🔧 Input tokens: torch.Size([1, 5])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 112])\n",
            "📖 Generated text length: 268 chars\n",
            "🎭 Extracted poem length: 268 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 사랑하는 동물|> \n",
            "당신이 나를 처음 만나\n",
            "당신에게\n",
            "당신이 나를 처음 만난 것처럼\n",
            "한 번쯤\n",
            "내 몸도 아프게 하고\n",
            "당신이 나를 처음 만난 것처럼\n",
            "당신의 몸도 아프게 한다\n",
            "당신이 나를 처음 만난 것처럼\n",
            "나를 처음 만난 것처럼\n",
            "당신은 나를 처음 만난 것처럼\n",
            "당신이 나를 처음 만난 것처럼\n",
            "당신이 나를 처음 만난 것처럼\n",
            "​\n",
            "당신이 나를 처음 만난 것처럼\n",
            "당신이 나를 처음 만난 것처럼\n",
            "당신이 나를 처음 만난 것처럼\n",
            "당신이 나를 처음 만난 것처럼\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 슬픈 사랑\n",
            "\n",
            "Generating poem for '슬픈 사랑'...\n",
            "🎯 Generating poem for topic: '슬픈 사랑'\n",
            "📝 Prompt: '<|topic:슬픈 사랑|>'\n",
            "🔧 Input tokens: torch.Size([1, 5])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 53])\n",
            "📖 Generated text length: 127 chars\n",
            "🎭 Extracted poem length: 127 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 슬픈 사랑|> \n",
            "내가 사랑했던 여인숙\n",
            "밤마다 꽃밭을 걷다가\n",
            "나는 그 여인숙에 살았다\n",
            "이제 나는 더 이상 그 여인숙에서 살지 않는다\n",
            "여인의 가슴속에\n",
            "내 눈물 같은 슬픈 사랑이 들어 있다 \n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 병에 걸린 물고기\n",
            "\n",
            "Generating poem for '병에 걸린 물고기'...\n",
            "🎯 Generating poem for topic: '병에 걸린 물고기'\n",
            "📝 Prompt: '<|topic:병에 걸린 물고기|>'\n",
            "🔧 Input tokens: torch.Size([1, 6])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 118])\n",
            "📖 Generated text length: 220 chars\n",
            "🎭 Extracted poem length: 220 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 병에 걸린 물고기|> \n",
            "시골집에 들어와\n",
            "비닐봉지 두 개를 들고 와\n",
            "물고기 두 마리를 잡수신다\n",
            "​\n",
            "그때, 어머니\n",
            "그분의 어머님은\n",
            "세상에 바쁘시다는 말씀을\n",
            "가르쳐 주셨다\n",
            "​\n",
            "어머니는 나를 뭍으로 데려와\n",
            "시골집 구석구석을 배웅하신다\n",
            "​\n",
            "어느덧 그분이 내 곁을 지나간다\n",
            "​\n",
            "어머니의 어머님은 내게 \"무얼 먹고 사는 게 좋으냐?\"라고\n",
            "물고기 두 마리를 잡수신다\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다위로 떠오르는 태양\n",
            "\n",
            "Generating poem for '바다위로 떠오르는 태양'...\n",
            "🎯 Generating poem for topic: '바다위로 떠오르는 태양'\n",
            "📝 Prompt: '<|topic:바다위로 떠오르는 태양|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 53])\n",
            "📖 Generated text length: 115 chars\n",
            "🎭 Extracted poem length: 115 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다위로 떠오르는 태양|> \n",
            "바다를 향해\n",
            "한 발 한 발\n",
            "바다로 가고 싶다\n",
            "​\n",
            "이제 너는\n",
            "바다로 가야 한다고\n",
            "나에게 말해주지만\n",
            "​\n",
            "그때 너는\n",
            "이 세상 어디에도 없다.\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다 위 태양\n",
            "\n",
            "Generating poem for '바다 위 태양'...\n",
            "🎯 Generating poem for topic: '바다 위 태양'\n",
            "📝 Prompt: '<|topic:바다 위 태양|>'\n",
            "🔧 Input tokens: torch.Size([1, 6])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 115])\n",
            "📖 Generated text length: 226 chars\n",
            "🎭 Extracted poem length: 226 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다 위 태양|> \n",
            "밤바다를 보며\n",
            "이렇게 말했지요\n",
            "소금 같은 바닷물이\n",
            "바다가 주는 밥과 같았지요\n",
            "​\n",
            "그런데 그게 다 내 것이 아니라\n",
            "내가 대신 받아먹는 것이었어요\n",
            "​\n",
            "그런데 알고 보니 그게 다 내 것이었어요\n",
            "​\n",
            "그러고 보면 바닷속에\n",
            "파도가 밀려와\n",
            "바다를 바라보고 있었지요\n",
            "​\n",
            "내가 대신 받아먹고\n",
            "내가 대신 받아먹고\n",
            "내가 대신 받아먹고\n",
            "내가 대신 받아먹고\n",
            "그랬지요\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다위로 떠오르는 태양\n",
            "\n",
            "Generating poem for '바다위로 떠오르는 태양'...\n",
            "🎯 Generating poem for topic: '바다위로 떠오르는 태양'\n",
            "📝 Prompt: '<|topic:바다위로 떠오르는 태양|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 137])\n",
            "📖 Generated text length: 228 chars\n",
            "🎭 Extracted poem length: 228 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다위로 떠오르는 태양|> \n",
            "해변가에는\n",
            "거친 파도 소리만\n",
            "아니다 \n",
            "​\n",
            "파도에 떠밀려 온 노파들이\n",
            "한껏 웅크린 채\n",
            "한숨을 쉬고 있다\n",
            "​\n",
            "노파의 거죽에\n",
            "수평선 위로 솟아오르는\n",
            "푸른 물결\n",
            "​\n",
            "노파의 거죽을\n",
            "한 번 더 핥아보려고\n",
            "바다는 한 번 더 출렁이고\n",
            "​\n",
            "노파의 거죽은\n",
            "거기다 가라앉아\n",
            "파도 소리로 출렁이고\n",
            "​\n",
            "노파의 거죽이 파도를 핥아보려고\n",
            "파도에 떠밀려온\n",
            "푸른 물결\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: 바다 위에 뜬 달\n",
            "\n",
            "Generating poem for '바다 위에 뜬 달'...\n",
            "🎯 Generating poem for topic: '바다 위에 뜬 달'\n",
            "📝 Prompt: '<|topic:바다 위에 뜬 달|>'\n",
            "🔧 Input tokens: torch.Size([1, 7])\n",
            "🔧 Device: cuda\n",
            "🎛️ Using Prompt-tuning model for generation\n",
            "✅ Generation completed. Output tokens: torch.Size([1, 113])\n",
            "📖 Generated text length: 197 chars\n",
            "🎭 Extracted poem length: 197 chars\n",
            "\n",
            "Generated poem:\n",
            "------------------------------\n",
            "<|topic: 바다 위에 뜬 달|> \n",
            "아득한 옛날이 있었지요\n",
            "바닷가의 한낮에도\n",
            "달빛은 따사로운 햇살을 뿌리며\n",
            "달빛과 바람 사이로 날아다녔지요\n",
            "​\n",
            "달빛에 비친 보름달의\n",
            "눈썹에 흰 별 하나가\n",
            "눈썹에 흰 별 하나가\n",
            "달빛을 반짝이는데\n",
            "​\n",
            "그런 밤이면\n",
            "달빛이\n",
            "달빛과 파도 사이를 날리고\n",
            "​\n",
            "그런 밤이면\n",
            "바다의 달빛은\n",
            "마음의 쉼표인 듯\n",
            "​<|endoftext|>\n",
            "------------------------------\n",
            "\n",
            "Enter topic: "
          ]
        }
      ],
      "source": [
        "!python3 main.py --mode generate --model-path /content/drive/MyDrive/2025-NLP/best_models/freeze-epoch5/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
