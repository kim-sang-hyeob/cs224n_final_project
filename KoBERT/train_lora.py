import os
import json
import numpy as np
import torch

from datasets import Dataset
import evaluate

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback,
    set_seed,
)

# -------------------------- ì„¤ì • --------------------------
# 1ë‹¨ê³„ì—ì„œ Fine-tuningëœ ì²´í¬í¬ì¸íŠ¸ í´ë” ê²½ë¡œ
# (ì˜ˆ: "kobert-emotion/checkpoint-27280" ë“±, ì‹¤ì œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)
BASE_CHECKPOINT = "kobert-emotion/checkpoint-27280"

# Full Fine-Tuning í›„ ì €ì¥í•  í´ë”
OUTPUT_DIR      = "./kobert-emotion-fullft-emotionjson"

# ì¶”ê°€ í•™ìŠµìš© ë¡œì»¬ JSON íŒŒì¼ ê²½ë¡œ
HQ_TRAIN_JSON   = "emotion_train_dataset.json"
HQ_VALID_JSON   = "emotion_valid_dataset.json"

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
MODEL_NAME   = "skt/kobert-base-v1"
MAX_LEN      = 128
BATCH_SIZE   = 8        # Full FT ì‹œ ë©”ëª¨ë¦¬ ì´ìŠˆê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 8 ì •ë„ë¡œ ì‹œì‘
LR           = 5e-6     # ì•„ì£¼ ì‘ì€ í•™ìŠµë¥  (5e-6 ~ 1e-6 ê¶Œì¥)
EPOCHS       = 7        # 2~3 ì—í¬í¬ ì •ë„ë¡œ ì‹œì‘, ê³¼ì í•© ì£¼ì˜
EARLY_STOPPING_PATIENCE = 1  # ê²€ì¦ F1ì´ 1 ì—í¬í¬ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
SEED         = 42

set_seed(SEED)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# -------------------- 1) ë ˆì´ë¸” ë§¤í•‘ (1ë‹¨ê³„ì™€ ë™ì¼) ----------------------
# ë°˜ë“œì‹œ 1ë‹¨ê³„ì™€ ê°™ì€ ìˆœì„œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
id2label = {
    0: "ë¶„ë…¸",
    1: "ìŠ¬í””",
    2: "ë¶ˆì•ˆ",
    3: "ë‹¹í™©",
    4: "ìƒì²˜",
    5: "ë¬´ê°ì •",
    6: "ê¸°ì¨",
}
label2id = {v: k for k, v in id2label.items()}

# -------------------- 2) ê³ í’ˆì§ˆ ë°ì´í„° ë¡œë“œ & Dataset ìƒì„± ----------------------
# emotion_train_dataset.json ê³¼ emotion_valid_dataset.json íŒŒì¼ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤:
# [
#   { "content": "ë¬¸ì¥1...", "emotion": "ìŠ¬í””" },
#   { "content": "ë¬¸ì¥2...", "emotion": "ê¸°ì¨" },
#   ...
# ]

with open(HQ_TRAIN_JSON, encoding="utf-8") as f:
    train_raw = json.load(f)

with open(HQ_VALID_JSON, encoding="utf-8") as f:
    valid_raw = json.load(f)

def convert_example(ex):
    return {
        "text":  ex["content"],
        "label": label2id[ex["emotion"]]
    }

train_list = [convert_example(x) for x in train_raw]
valid_list = [convert_example(x) for x in valid_raw]

train_ds = Dataset.from_list(train_list)
valid_ds = Dataset.from_list(valid_list)

# -------------------- 3) í† í¬ë‚˜ì´ì € & í† í°í™” ----------------------------
tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)

def tokenize_fn(batch):
    enc = tok(
        batch["text"],
        truncation=True,
        max_length=MAX_LEN,
        padding=False,            # DataCollatorWithPaddingê°€ íŒ¨ë”© ì²˜ë¦¬
        return_token_type_ids=False
    )
    enc.pop("token_type_ids", None)
    return enc

# "text" ì»¬ëŸ¼ì„ ì—†ì• ê³ , í† í°í™”ëœ ê²°ê³¼(input_ids, attention_mask)ë§Œ ë‚¨ê¹€
train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=["text"])
valid_ds = valid_ds.map(tokenize_fn, batched=True, remove_columns=["text"])

# í•„ìš”í•œ ì»¬ëŸ¼(input_ids, attention_mask, label)ë§Œ tensorë¡œ ë³€í™˜
train_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
valid_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

data_collator = DataCollatorWithPadding(tok, pad_to_multiple_of=8)

# -------------------- 4) 1ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (Full Fine-Tuning) ----------------------------
# 1ë‹¨ê³„ í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ ë’¤, ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
base_model = AutoModelForSequenceClassification.from_pretrained(
    BASE_CHECKPOINT,
    num_labels   = len(id2label),  # 7ê°œ í´ë˜ìŠ¤
    id2label     = id2label,
    label2id     = label2id,
)

# ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì • (requires_grad=True)
for param in base_model.parameters():
    param.requires_grad = True

# -------------------- 5) í‰ê°€ ì§€í‘œ ì •ì˜ ----------------------------
metric_acc = evaluate.load("accuracy")
metric_f1  = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": metric_acc.compute(predictions=preds, references=labels)["accuracy"],
        "macro_f1": metric_f1.compute(predictions=preds, references=labels, average="macro")["f1"],
    }

# -------------------- 6) TrainingArguments & Trainer -------------------
training_args = TrainingArguments(
    output_dir               = OUTPUT_DIR,
    evaluation_strategy      = "epoch",
    save_strategy            = "epoch",
    load_best_model_at_end   = True,
    metric_for_best_model    = "macro_f1",
    greater_is_better        = True,

    num_train_epochs         = EPOCHS,
    per_device_train_batch_size  = BATCH_SIZE,
    per_device_eval_batch_size   = BATCH_SIZE,
    learning_rate            = LR,
    weight_decay             = 0.01,
    fp16                     = torch.cuda.is_available(),

    report_to                = "none",
    seed                     = SEED,
    save_total_limit         = 2,
)

trainer = Trainer(
    model           = base_model,
    args            = training_args,
    train_dataset   = train_ds,
    eval_dataset    = valid_ds,
    tokenizer       = tok,
    data_collator   = data_collator,
    compute_metrics = compute_metrics,
    callbacks       = [EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)],
)

# -------------------- 7) í•™ìŠµ ë° ì €ì¥ --------------------------
if __name__ == "__main__":
    print(f"â–¶ï¸ ê³ í’ˆì§ˆ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_ds)}")
    print(f"â–¶ï¸ ê³ í’ˆì§ˆ ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(valid_ds)}")

    print("\nğŸš€ Full Fine-Tuning (ì¶”ê°€ í•™ìŠµ) ì‹œì‘...\n")
    trainer.train()

    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model(OUTPUT_DIR)
    tok.save_pretrained(OUTPUT_DIR)

    saved_files = os.listdir(OUTPUT_DIR)
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ! '{OUTPUT_DIR}' í´ë” ë‚´ íŒŒì¼ ëª©ë¡:")
    for fn in sorted(saved_files):
        print("   â€¢", fn)
    print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {os.path.abspath(OUTPUT_DIR)}")
